{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkDNnmZK8Bu7IJwOm+UOai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anidafio/4105_hw7/blob/main/hw7_p1a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWgkc9WHw9zw",
        "outputId": "fe37e00c-84c9-401d-ffc7-84144e282acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 47235679.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: ./data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                             (0.5, 0.5, 0.5))\n",
        "    ]))\n",
        "\n",
        "cifar10_val = datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                             (0.5, 0.5, 0.5))\n",
        "    ]))\n",
        "\n",
        "cifar10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.act1 = nn.Tanh()\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.act2 = nn.Tanh()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.act3 = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3, 32, 32)\n",
        "\n",
        "        out = self.pool1(self.act1(self.conv1(x)))\n",
        "        out = self.pool2(self.act2(self.conv2(out)))\n",
        "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
        "        out = self.act3(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "seq_model = Net()\n",
        "\n",
        "optimizer = optim.Adam(seq_model.parameters(), lr=1e-3) # <1>\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "n_epochs = 300\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = seq_model(imgs.view(imgs.shape[0], -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD58A7Acz1fu",
        "outputId": "88dd40ca-4fd1-4550-f37f-8ad032d35a53"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.003250\n",
            "Epoch: 1, Loss: 1.807877\n",
            "Epoch: 2, Loss: 1.645387\n",
            "Epoch: 3, Loss: 1.494908\n",
            "Epoch: 4, Loss: 1.367699\n",
            "Epoch: 5, Loss: 1.244117\n",
            "Epoch: 6, Loss: 1.155466\n",
            "Epoch: 7, Loss: 1.101465\n",
            "Epoch: 8, Loss: 1.062641\n",
            "Epoch: 9, Loss: 1.031834\n",
            "Epoch: 10, Loss: 0.996910\n",
            "Epoch: 11, Loss: 0.963931\n",
            "Epoch: 12, Loss: 0.932193\n",
            "Epoch: 13, Loss: 0.900360\n",
            "Epoch: 14, Loss: 0.867656\n",
            "Epoch: 15, Loss: 0.841149\n",
            "Epoch: 16, Loss: 0.820506\n",
            "Epoch: 17, Loss: 0.805178\n",
            "Epoch: 18, Loss: 0.792155\n",
            "Epoch: 19, Loss: 0.778684\n",
            "Epoch: 20, Loss: 0.765357\n",
            "Epoch: 21, Loss: 0.755302\n",
            "Epoch: 22, Loss: 0.748296\n",
            "Epoch: 23, Loss: 0.739864\n",
            "Epoch: 24, Loss: 0.731695\n",
            "Epoch: 25, Loss: 0.726385\n",
            "Epoch: 26, Loss: 0.719498\n",
            "Epoch: 27, Loss: 0.716359\n",
            "Epoch: 28, Loss: 0.709512\n",
            "Epoch: 29, Loss: 0.702552\n",
            "Epoch: 30, Loss: 0.694284\n",
            "Epoch: 31, Loss: 0.684283\n",
            "Epoch: 32, Loss: 0.674217\n",
            "Epoch: 33, Loss: 0.664714\n",
            "Epoch: 34, Loss: 0.653464\n",
            "Epoch: 35, Loss: 0.637832\n",
            "Epoch: 36, Loss: 0.624889\n",
            "Epoch: 37, Loss: 0.610369\n",
            "Epoch: 38, Loss: 0.598389\n",
            "Epoch: 39, Loss: 0.587996\n",
            "Epoch: 40, Loss: 0.581653\n",
            "Epoch: 41, Loss: 0.578723\n",
            "Epoch: 42, Loss: 0.577803\n",
            "Epoch: 43, Loss: 0.577043\n",
            "Epoch: 44, Loss: 0.577763\n",
            "Epoch: 45, Loss: 0.577731\n",
            "Epoch: 46, Loss: 0.578180\n",
            "Epoch: 47, Loss: 0.577344\n",
            "Epoch: 48, Loss: 0.579456\n",
            "Epoch: 49, Loss: 0.580457\n",
            "Epoch: 50, Loss: 0.579112\n",
            "Epoch: 51, Loss: 0.572816\n",
            "Epoch: 52, Loss: 0.565629\n",
            "Epoch: 53, Loss: 0.558499\n",
            "Epoch: 54, Loss: 0.544903\n",
            "Epoch: 55, Loss: 0.530960\n",
            "Epoch: 56, Loss: 0.515449\n",
            "Epoch: 57, Loss: 0.494376\n",
            "Epoch: 58, Loss: 0.470765\n",
            "Epoch: 59, Loss: 0.451450\n",
            "Epoch: 60, Loss: 0.426558\n",
            "Epoch: 61, Loss: 0.417369\n",
            "Epoch: 62, Loss: 0.422806\n",
            "Epoch: 63, Loss: 0.442816\n",
            "Epoch: 64, Loss: 0.459650\n",
            "Epoch: 65, Loss: 0.467287\n",
            "Epoch: 66, Loss: 0.471826\n",
            "Epoch: 67, Loss: 0.470889\n",
            "Epoch: 68, Loss: 0.468501\n",
            "Epoch: 69, Loss: 0.461418\n",
            "Epoch: 70, Loss: 0.452994\n",
            "Epoch: 71, Loss: 0.449058\n",
            "Epoch: 72, Loss: 0.444627\n",
            "Epoch: 73, Loss: 0.437926\n",
            "Epoch: 74, Loss: 0.429708\n",
            "Epoch: 75, Loss: 0.419710\n",
            "Epoch: 76, Loss: 0.411002\n",
            "Epoch: 77, Loss: 0.405872\n",
            "Epoch: 78, Loss: 0.401284\n",
            "Epoch: 79, Loss: 0.398166\n",
            "Epoch: 80, Loss: 0.393057\n",
            "Epoch: 81, Loss: 0.393024\n",
            "Epoch: 82, Loss: 0.390551\n",
            "Epoch: 83, Loss: 0.385412\n",
            "Epoch: 84, Loss: 0.381086\n",
            "Epoch: 85, Loss: 0.372857\n",
            "Epoch: 86, Loss: 0.375102\n",
            "Epoch: 87, Loss: 0.379506\n",
            "Epoch: 88, Loss: 0.372642\n",
            "Epoch: 89, Loss: 0.365253\n",
            "Epoch: 90, Loss: 0.357151\n",
            "Epoch: 91, Loss: 0.351744\n",
            "Epoch: 92, Loss: 0.348935\n",
            "Epoch: 93, Loss: 0.344596\n",
            "Epoch: 94, Loss: 0.340152\n",
            "Epoch: 95, Loss: 0.335369\n",
            "Epoch: 96, Loss: 0.331863\n",
            "Epoch: 97, Loss: 0.330281\n",
            "Epoch: 98, Loss: 0.332397\n",
            "Epoch: 99, Loss: 0.336218\n",
            "Epoch: 100, Loss: 0.342202\n",
            "Epoch: 101, Loss: 0.350658\n",
            "Epoch: 102, Loss: 0.361443\n",
            "Epoch: 103, Loss: 0.372111\n",
            "Epoch: 104, Loss: 0.379996\n",
            "Epoch: 105, Loss: 0.381120\n",
            "Epoch: 106, Loss: 0.376289\n",
            "Epoch: 107, Loss: 0.386728\n",
            "Epoch: 108, Loss: 0.392750\n",
            "Epoch: 109, Loss: 0.401926\n",
            "Epoch: 110, Loss: 0.410467\n",
            "Epoch: 111, Loss: 0.410052\n",
            "Epoch: 112, Loss: 0.409253\n",
            "Epoch: 113, Loss: 0.403848\n",
            "Epoch: 114, Loss: 0.405263\n",
            "Epoch: 115, Loss: 0.401221\n",
            "Epoch: 116, Loss: 0.402358\n",
            "Epoch: 117, Loss: 0.399798\n",
            "Epoch: 118, Loss: 0.403284\n",
            "Epoch: 119, Loss: 0.410719\n",
            "Epoch: 120, Loss: 0.412569\n",
            "Epoch: 121, Loss: 0.418328\n",
            "Epoch: 122, Loss: 0.420825\n",
            "Epoch: 123, Loss: 0.419659\n",
            "Epoch: 124, Loss: 0.421701\n",
            "Epoch: 125, Loss: 0.416547\n",
            "Epoch: 126, Loss: 0.396193\n",
            "Epoch: 127, Loss: 0.383600\n",
            "Epoch: 128, Loss: 0.381132\n",
            "Epoch: 129, Loss: 0.403931\n",
            "Epoch: 130, Loss: 0.403426\n",
            "Epoch: 131, Loss: 0.414092\n",
            "Epoch: 132, Loss: 0.380475\n",
            "Epoch: 133, Loss: 0.379779\n",
            "Epoch: 134, Loss: 0.355717\n",
            "Epoch: 135, Loss: 0.363005\n",
            "Epoch: 136, Loss: 0.348215\n",
            "Epoch: 137, Loss: 0.355363\n",
            "Epoch: 138, Loss: 0.345940\n",
            "Epoch: 139, Loss: 0.345782\n",
            "Epoch: 140, Loss: 0.343667\n",
            "Epoch: 141, Loss: 0.342526\n",
            "Epoch: 142, Loss: 0.345015\n",
            "Epoch: 143, Loss: 0.344862\n",
            "Epoch: 144, Loss: 0.345481\n",
            "Epoch: 145, Loss: 0.341738\n",
            "Epoch: 146, Loss: 0.337476\n",
            "Epoch: 147, Loss: 0.342761\n",
            "Epoch: 148, Loss: 0.347176\n",
            "Epoch: 149, Loss: 0.353932\n",
            "Epoch: 150, Loss: 0.355153\n",
            "Epoch: 151, Loss: 0.359040\n",
            "Epoch: 152, Loss: 0.364199\n",
            "Epoch: 153, Loss: 0.359958\n",
            "Epoch: 154, Loss: 0.358822\n",
            "Epoch: 155, Loss: 0.367642\n",
            "Epoch: 156, Loss: 0.373097\n",
            "Epoch: 157, Loss: 0.364203\n",
            "Epoch: 158, Loss: 0.373029\n",
            "Epoch: 159, Loss: 0.388954\n",
            "Epoch: 160, Loss: 0.382098\n",
            "Epoch: 161, Loss: 0.380143\n",
            "Epoch: 162, Loss: 0.379024\n",
            "Epoch: 163, Loss: 0.379758\n",
            "Epoch: 164, Loss: 0.377278\n",
            "Epoch: 165, Loss: 0.379993\n",
            "Epoch: 166, Loss: 0.379917\n",
            "Epoch: 167, Loss: 0.377045\n",
            "Epoch: 168, Loss: 0.372998\n",
            "Epoch: 169, Loss: 0.378804\n",
            "Epoch: 170, Loss: 0.393005\n",
            "Epoch: 171, Loss: 0.392192\n",
            "Epoch: 172, Loss: 0.381674\n",
            "Epoch: 173, Loss: 0.364369\n",
            "Epoch: 174, Loss: 0.357825\n",
            "Epoch: 175, Loss: 0.350861\n",
            "Epoch: 176, Loss: 0.353034\n",
            "Epoch: 177, Loss: 0.377062\n",
            "Epoch: 178, Loss: 0.408075\n",
            "Epoch: 179, Loss: 0.447063\n",
            "Epoch: 180, Loss: 0.477865\n",
            "Epoch: 181, Loss: 0.492345\n",
            "Epoch: 182, Loss: 0.464052\n",
            "Epoch: 183, Loss: 0.436964\n",
            "Epoch: 184, Loss: 0.433855\n",
            "Epoch: 185, Loss: 0.401516\n",
            "Epoch: 186, Loss: 0.382796\n",
            "Epoch: 187, Loss: 0.378233\n",
            "Epoch: 188, Loss: 0.364096\n",
            "Epoch: 189, Loss: 0.370042\n",
            "Epoch: 190, Loss: 0.361007\n",
            "Epoch: 191, Loss: 0.357134\n",
            "Epoch: 192, Loss: 0.353953\n",
            "Epoch: 193, Loss: 0.358935\n",
            "Epoch: 194, Loss: 0.366015\n",
            "Epoch: 195, Loss: 0.360126\n",
            "Epoch: 196, Loss: 0.345915\n",
            "Epoch: 197, Loss: 0.353502\n",
            "Epoch: 198, Loss: 0.398215\n",
            "Epoch: 199, Loss: 0.399028\n",
            "Epoch: 200, Loss: 0.429467\n",
            "Epoch: 201, Loss: 0.476281\n",
            "Epoch: 202, Loss: 0.470269\n",
            "Epoch: 203, Loss: 0.448305\n",
            "Epoch: 204, Loss: 0.410491\n",
            "Epoch: 205, Loss: 0.379252\n",
            "Epoch: 206, Loss: 0.373893\n",
            "Epoch: 207, Loss: 0.376074\n",
            "Epoch: 208, Loss: 0.373885\n",
            "Epoch: 209, Loss: 0.366977\n",
            "Epoch: 210, Loss: 0.373228\n",
            "Epoch: 211, Loss: 0.363029\n",
            "Epoch: 212, Loss: 0.351348\n",
            "Epoch: 213, Loss: 0.356498\n",
            "Epoch: 214, Loss: 0.337416\n",
            "Epoch: 215, Loss: 0.324586\n",
            "Epoch: 216, Loss: 0.318266\n",
            "Epoch: 217, Loss: 0.319233\n",
            "Epoch: 218, Loss: 0.332130\n",
            "Epoch: 219, Loss: 0.338172\n",
            "Epoch: 220, Loss: 0.355440\n",
            "Epoch: 221, Loss: 0.348461\n",
            "Epoch: 222, Loss: 0.322886\n",
            "Epoch: 223, Loss: 0.310358\n",
            "Epoch: 224, Loss: 0.300062\n",
            "Epoch: 225, Loss: 0.298665\n",
            "Epoch: 226, Loss: 0.282709\n",
            "Epoch: 227, Loss: 0.284472\n",
            "Epoch: 228, Loss: 0.276504\n",
            "Epoch: 229, Loss: 0.281936\n",
            "Epoch: 230, Loss: 0.286238\n",
            "Epoch: 231, Loss: 0.299448\n",
            "Epoch: 232, Loss: 0.294395\n",
            "Epoch: 233, Loss: 0.288914\n",
            "Epoch: 234, Loss: 0.293212\n",
            "Epoch: 235, Loss: 0.317055\n",
            "Epoch: 236, Loss: 0.297331\n",
            "Epoch: 237, Loss: 0.300062\n",
            "Epoch: 238, Loss: 0.282364\n",
            "Epoch: 239, Loss: 0.264695\n",
            "Epoch: 240, Loss: 0.265642\n",
            "Epoch: 241, Loss: 0.265445\n",
            "Epoch: 242, Loss: 0.277455\n",
            "Epoch: 243, Loss: 0.260744\n",
            "Epoch: 244, Loss: 0.252476\n",
            "Epoch: 245, Loss: 0.248408\n",
            "Epoch: 246, Loss: 0.250564\n",
            "Epoch: 247, Loss: 0.238703\n",
            "Epoch: 248, Loss: 0.232905\n",
            "Epoch: 249, Loss: 0.243631\n",
            "Epoch: 250, Loss: 0.240384\n",
            "Epoch: 251, Loss: 0.254131\n",
            "Epoch: 252, Loss: 0.254195\n",
            "Epoch: 253, Loss: 0.250376\n",
            "Epoch: 254, Loss: 0.262530\n",
            "Epoch: 255, Loss: 0.266396\n",
            "Epoch: 256, Loss: 0.290280\n",
            "Epoch: 257, Loss: 0.265068\n",
            "Epoch: 258, Loss: 0.261115\n",
            "Epoch: 259, Loss: 0.277805\n",
            "Epoch: 260, Loss: 0.260235\n",
            "Epoch: 261, Loss: 0.274020\n",
            "Epoch: 262, Loss: 0.267380\n",
            "Epoch: 263, Loss: 0.274912\n",
            "Epoch: 264, Loss: 0.281396\n",
            "Epoch: 265, Loss: 0.258502\n",
            "Epoch: 266, Loss: 0.267686\n",
            "Epoch: 267, Loss: 0.242672\n",
            "Epoch: 268, Loss: 0.220027\n",
            "Epoch: 269, Loss: 0.217733\n",
            "Epoch: 270, Loss: 0.221552\n",
            "Epoch: 271, Loss: 0.216309\n",
            "Epoch: 272, Loss: 0.203956\n",
            "Epoch: 273, Loss: 0.196373\n",
            "Epoch: 274, Loss: 0.209127\n",
            "Epoch: 275, Loss: 0.238063\n",
            "Epoch: 276, Loss: 0.215413\n",
            "Epoch: 277, Loss: 0.268908\n",
            "Epoch: 278, Loss: 0.261117\n",
            "Epoch: 279, Loss: 0.271540\n",
            "Epoch: 280, Loss: 0.274452\n",
            "Epoch: 281, Loss: 0.296109\n",
            "Epoch: 282, Loss: 0.247743\n",
            "Epoch: 283, Loss: 0.269663\n",
            "Epoch: 284, Loss: 0.283192\n",
            "Epoch: 285, Loss: 0.268124\n",
            "Epoch: 286, Loss: 0.231245\n",
            "Epoch: 287, Loss: 0.210859\n",
            "Epoch: 288, Loss: 0.204131\n",
            "Epoch: 289, Loss: 0.226498\n",
            "Epoch: 290, Loss: 0.215087\n",
            "Epoch: 291, Loss: 0.205268\n",
            "Epoch: 292, Loss: 0.199810\n",
            "Epoch: 293, Loss: 0.194319\n",
            "Epoch: 294, Loss: 0.212670\n",
            "Epoch: 295, Loss: 0.255085\n",
            "Epoch: 296, Loss: 0.239998\n",
            "Epoch: 297, Loss: 0.262451\n",
            "Epoch: 298, Loss: 0.274592\n",
            "Epoch: 299, Loss: 0.285292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = seq_model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr-ATDyAgeqb",
        "outputId": "249cab68-634d-4cc6-e6f3-c559ebff96b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.726220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = seq_model(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ18mbiLgzZR",
        "outputId": "a3161852-7bab-4494-b52b-9cd2c3075190"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.568300\n"
          ]
        }
      ]
    }
  ]
}